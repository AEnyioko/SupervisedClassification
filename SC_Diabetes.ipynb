{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f09eb0",
   "metadata": {},
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Chibuzor Enyioko\" -d -v -p numpy,pandas,matplotlib,seaborn,sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d733ab",
   "metadata": {},
   "source": [
    "# Project 2: Supervised Classification\n",
    "\n",
    "This project uses python packages to perform different unsupervised learning methods on a given breast cancer and diabetes dataset.\n",
    "\n",
    "## Part 2: Diabetes Dataset\n",
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2ea2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3e0888",
   "metadata": {},
   "source": [
    "### Problems\n",
    "1. Identify which column(s) (both train and test) has/have missing values? Identify the ‘row id’s.\n",
    "“Impute” them with “Average/Most Frequent” values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49aeaa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "# importing data sets\n",
    "\n",
    "dbt_training_data = pd.read_csv(\"diabetes_training.csv\")\n",
    "dbt_test_data = pd.read_csv(\"diabetes_testing.csv\")\n",
    "\n",
    "x_train = dbt_training_data.drop(columns=['id', 'class'])\n",
    "y_train = dbt_training_data['class']\n",
    "x_test = dbt_test_data.drop(columns=['id', 'class'])\n",
    "y_test = dbt_test_data['class']\n",
    "\n",
    "#imputer strategy to k nearest neighbors\n",
    "\n",
    "knn_columns = ['plas','pres','skin','insu','mass','pedi','age']\n",
    "imputer_knn = KNNImputer(n_neighbors=3)\n",
    "\n",
    "x_train_knn = x_train[knn_columns].copy()\n",
    "x_train_knn.replace(0, np.nan, inplace=True)\n",
    "x_test_knn = x_test[knn_columns].copy()\n",
    "x_test_knn.replace(0, np.nan, inplace=True)\n",
    "\n",
    "train_imputed_knn = pd.DataFrame(imputer_knn.fit_transform(x_train_knn), columns=knn_columns)\n",
    "test_imputed_knn = pd.DataFrame(imputer_knn.transform(x_test_knn), columns=knn_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f17a46c",
   "metadata": {},
   "source": [
    "2. Calculate accuracy using each of these classifiers (up to 3 decimal places):\n",
    "\n",
    "3. Now tweak the parameters of the above models, what is the best result you can get? Write the answer and upload the workbook as proof. Name this classifier widget as “<classifier>-best”. Example (if the tree widget is the best performer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80822986",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "684365e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.779     0.870     0.822       146\n",
      "tested_positive      0.721     0.576     0.641        85\n",
      "\n",
      "       accuracy                          0.762       231\n",
      "      macro avg      0.750     0.723     0.731       231\n",
      "   weighted avg      0.758     0.762     0.755       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "x_train = train_imputed_knn\n",
    "x_test = test_imputed_knn\n",
    "\n",
    "\n",
    "clf = LogisticRegression(penalty='l2', C=0.5, max_iter=1000)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Metrics\n",
    "y_pred = clf.predict(x_test)\n",
    "print(classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5bd072fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.708     0.863     0.778       146\n",
      "tested_positive      0.623     0.388     0.478        85\n",
      "\n",
      "       accuracy                          0.688       231\n",
      "      macro avg      0.665     0.626     0.628       231\n",
      "   weighted avg      0.677     0.688     0.668       231\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aenyioko/miniconda3/envs/sklearn-env/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tweaking parameters for Logistic Regression\n",
    "clf_best = LogisticRegression(penalty='l1', solver='saga', C=7, max_iter=100)\n",
    "clf_best.fit(x_train, y_train)\n",
    "y_pred_best = clf_best.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_best, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ef67e",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dc98048c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.787     0.836     0.811       146\n",
      "tested_positive      0.684     0.612     0.646        85\n",
      "\n",
      "       accuracy                          0.753       231\n",
      "      macro avg      0.736     0.724     0.728       231\n",
      "   weighted avg      0.749     0.753     0.750       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_pred_gnb = gnb.fit(x_train, y_train).predict(x_test)\n",
    "print(classification_report(y_test, y_pred_gnb, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f3001",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a83f7dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.734     0.699     0.716       146\n",
      "tested_positive      0.522     0.565     0.542        85\n",
      "\n",
      "       accuracy                          0.649       231\n",
      "      macro avg      0.628     0.632     0.629       231\n",
      "   weighted avg      0.656     0.649     0.652       231\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aenyioko/miniconda3/envs/sklearn-env/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn import svm\n",
    "\n",
    "svm_clf = svm.SVC(C=1.0, kernel='rbf', gamma='auto', max_iter=100)\n",
    "svm_clf.fit(x_train, y_train)\n",
    "y_pred_svm = svm_clf.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_svm, zero_division=0, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "689b6726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.759     0.904     0.825       146\n",
      "tested_positive      0.754     0.506     0.606        85\n",
      "\n",
      "       accuracy                          0.758       231\n",
      "      macro avg      0.757     0.705     0.715       231\n",
      "   weighted avg      0.757     0.758     0.744       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tweaking parameters for SVM\n",
    "svm_clf_best = svm.SVC(C=1.2, kernel='poly', gamma='scale', max_iter=-1)\n",
    "svm_clf_best.fit(x_train, y_train)\n",
    "y_pred_svm_best = svm_clf_best.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_svm_best, zero_division=0, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039dac8c",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "baad9a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.771     0.829     0.799       146\n",
      "tested_positive      0.662     0.576     0.616        85\n",
      "\n",
      "       accuracy                          0.736       231\n",
      "      macro avg      0.716     0.703     0.708       231\n",
      "   weighted avg      0.731     0.736     0.732       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=10, min_samples_split=5)\n",
    "rf_clf.fit(x_train, y_train)\n",
    "y_pred_rf = rf_clf.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_rf, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f2114c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.796     0.801     0.799       146\n",
      "tested_positive      0.655     0.647     0.651        85\n",
      "\n",
      "       accuracy                          0.745       231\n",
      "      macro avg      0.725     0.724     0.725       231\n",
      "   weighted avg      0.744     0.745     0.744       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tweaking random forest parameters\n",
    "rf_clf_best = RandomForestClassifier(n_estimators=50, min_samples_split=10)\n",
    "rf_clf_best.fit(x_train, y_train)\n",
    "y_pred_rf_best = rf_clf_best.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_rf_best, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaea4121",
   "metadata": {},
   "source": [
    "#### k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ecc9ff1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.755     0.801     0.777       146\n",
      "tested_positive      0.618     0.553     0.584        85\n",
      "\n",
      "       accuracy                          0.710       231\n",
      "      macro avg      0.687     0.677     0.681       231\n",
      "   weighted avg      0.705     0.710     0.706       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k-Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5, metric='euclidean', weights='uniform')\n",
    "knn_clf.fit(x_train, y_train)\n",
    "y_pred_knn = knn_clf.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_knn, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00632ca",
   "metadata": {},
   "source": [
    "#### Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bddbed1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.763     0.815     0.788       146\n",
      "tested_positive      0.640     0.565     0.600        85\n",
      "\n",
      "       accuracy                          0.723       231\n",
      "      macro avg      0.701     0.690     0.694       231\n",
      "   weighted avg      0.718     0.723     0.719       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_clf = DecisionTreeClassifier(max_depth=5, min_samples_split=5, min_samples_leaf=2)\n",
    "tree_clf.fit(x_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_tree, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
