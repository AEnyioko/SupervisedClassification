{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f09eb0",
   "metadata": {},
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Chibuzor Enyioko\" -d -v -p numpy,pandas,matplotlib,seaborn,sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d733ab",
   "metadata": {},
   "source": [
    "# Project 2: Supervised Classification\n",
    "\n",
    "This project uses python packages to perform different unsupervised learning methods on a given breast cancer and diabetes dataset.\n",
    "\n",
    "## Part 2: Diabetes Dataset\n",
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2ea2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3e0888",
   "metadata": {},
   "source": [
    "### Problems\n",
    "1. Identify which column(s) (both train and test) has/have missing values? Identify the ‘row id’s.\n",
    "“Impute” them with “Average/Most Frequent” values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49aeaa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "# importing data sets\n",
    "\n",
    "dbt_training_data = pd.read_csv(\"diabetes_training.csv\")\n",
    "dbt_test_data = pd.read_csv(\"diabetes_testing.csv\")\n",
    "\n",
    "x_train = dbt_training_data.drop(columns=['id', 'class'])\n",
    "y_train = dbt_training_data['class']\n",
    "x_test = dbt_test_data.drop(columns=['id', 'class'])\n",
    "y_test = dbt_test_data['class']\n",
    "\n",
    "#imputer strategy to k nearest neighbors\n",
    "\n",
    "knn_columns = ['plas','pres','skin','insu','mass','pedi','age']\n",
    "imputer_knn = KNNImputer(n_neighbors=3)\n",
    "\n",
    "x_train_knn = x_train[knn_columns].copy()\n",
    "x_train_knn.replace(0, np.nan, inplace=True)\n",
    "x_test_knn = x_test[knn_columns].copy()\n",
    "x_test_knn.replace(0, np.nan, inplace=True)\n",
    "\n",
    "train_imputed_knn = pd.DataFrame(imputer_knn.fit_transform(x_train_knn), columns=knn_columns)\n",
    "test_imputed_knn = pd.DataFrame(imputer_knn.transform(x_test_knn), columns=knn_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f17a46c",
   "metadata": {},
   "source": [
    "2. Calculate accuracy using each of these classifiers (up to 3 decimal places):\n",
    "\n",
    "3. Now tweak the parameters of the above models, what is the best result you can get? Write the answer and upload the workbook as proof. Name this classifier widget as “<classifier>-best”. Example (if the tree widget is the best performer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80822986",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "684365e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.779     0.870     0.822       146\n",
      "tested_positive      0.721     0.576     0.641        85\n",
      "\n",
      "       accuracy                          0.762       231\n",
      "      macro avg      0.750     0.723     0.731       231\n",
      "   weighted avg      0.758     0.762     0.755       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "x_train = train_imputed_knn\n",
    "x_test = test_imputed_knn\n",
    "\n",
    "\n",
    "clf = LogisticRegression(penalty='l2', C=0.5, max_iter=1000)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Metrics\n",
    "y_pred = clf.predict(x_test)\n",
    "print(classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5bd072fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.791     0.884     0.835       146\n",
      "tested_positive      0.750     0.600     0.667        85\n",
      "\n",
      "       accuracy                          0.779       231\n",
      "      macro avg      0.771     0.742     0.751       231\n",
      "   weighted avg      0.776     0.779     0.773       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tweaking parameters for Logistic Regression\n",
    "clf_best = LogisticRegression(penalty='l2', solver='liblinear', C=5, max_iter=1000)\n",
    "clf_best.fit(x_train, y_train)\n",
    "y_pred_best = clf_best.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_best, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ef67e",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc98048c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.787     0.836     0.811       146\n",
      "tested_positive      0.684     0.612     0.646        85\n",
      "\n",
      "       accuracy                          0.753       231\n",
      "      macro avg      0.736     0.724     0.728       231\n",
      "   weighted avg      0.749     0.753     0.750       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_pred_gnb = gnb.fit(x_train, y_train).predict(x_test)\n",
    "print(classification_report(y_test, y_pred_gnb, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f3001",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a83f7dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.734     0.699     0.716       146\n",
      "tested_positive      0.522     0.565     0.542        85\n",
      "\n",
      "       accuracy                          0.649       231\n",
      "      macro avg      0.628     0.632     0.629       231\n",
      "   weighted avg      0.656     0.649     0.652       231\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acenyiokopc/miniconda3/envs/sklearn/lib/python3.13/site-packages/sklearn/svm/_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn import svm\n",
    "\n",
    "svm_clf = svm.SVC(C=1.0, kernel='rbf', gamma='auto', max_iter=100)\n",
    "svm_clf.fit(x_train, y_train)\n",
    "y_pred_svm = svm_clf.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_svm, zero_division=0, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "689b6726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.764     0.911     0.831       146\n",
      "tested_positive      0.772     0.518     0.620        85\n",
      "\n",
      "       accuracy                          0.766       231\n",
      "      macro avg      0.768     0.714     0.725       231\n",
      "   weighted avg      0.767     0.766     0.753       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tweaking parameters for SVM\n",
    "svm_clf_best = svm.SVC(C=7, kernel='poly', gamma='scale', max_iter=-1)\n",
    "svm_clf_best.fit(x_train, y_train)\n",
    "y_pred_svm_best = svm_clf_best.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_svm_best, zero_division=0, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039dac8c",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad9a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.778     0.890     0.831       146\n",
      "tested_positive      0.750     0.565     0.644        85\n",
      "\n",
      "       accuracy                          0.771       231\n",
      "      macro avg      0.764     0.728     0.737       231\n",
      "   weighted avg      0.768     0.771     0.762       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=50, min_samples_split=10)\n",
    "rf_clf.fit(x_train, y_train)\n",
    "y_pred_rf = rf_clf.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_rf, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f2114c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.784     0.822     0.803       146\n",
      "tested_positive      0.667     0.612     0.638        85\n",
      "\n",
      "       accuracy                          0.745       231\n",
      "      macro avg      0.725     0.717     0.720       231\n",
      "   weighted avg      0.741     0.745     0.742       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tweaking random forest parameters\n",
    "rf_clf_best = RandomForestClassifier(n_estimators=5, min_samples_split=10, max_depth=3)\n",
    "rf_clf_best.fit(x_train, y_train)\n",
    "y_pred_rf_best = rf_clf_best.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_rf_best, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaea4121",
   "metadata": {},
   "source": [
    "#### k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc9ff1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.755     0.801     0.777       146\n",
      "tested_positive      0.618     0.553     0.584        85\n",
      "\n",
      "       accuracy                          0.710       231\n",
      "      macro avg      0.687     0.677     0.681       231\n",
      "   weighted avg      0.705     0.710     0.706       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k-Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5, metric='euclidean', weights='uniform')\n",
    "knn_clf.fit(x_train, y_train)\n",
    "y_pred_knn = knn_clf.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_knn, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1a47968c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.785     0.849     0.816       146\n",
      "tested_positive      0.699     0.600     0.646        85\n",
      "\n",
      "       accuracy                          0.758       231\n",
      "      macro avg      0.742     0.725     0.731       231\n",
      "   weighted avg      0.753     0.758     0.753       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tweaking k-NN parameters\n",
    "knn_clf_best = KNeighborsClassifier(n_neighbors=17, metric='euclidean', weights='distance', algorithm='auto')\n",
    "knn_clf_best.fit(x_train, y_train)\n",
    "y_pred_knn = knn_clf_best.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_knn, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00632ca",
   "metadata": {},
   "source": [
    "#### Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bddbed1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.763     0.815     0.788       146\n",
      "tested_positive      0.640     0.565     0.600        85\n",
      "\n",
      "       accuracy                          0.723       231\n",
      "      macro avg      0.701     0.690     0.694       231\n",
      "   weighted avg      0.718     0.723     0.719       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_clf = DecisionTreeClassifier(max_depth=5, min_samples_split=5, min_samples_leaf=2)\n",
    "tree_clf.fit(x_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_tree, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a5c93cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "tested_negative      0.758     0.774     0.766       146\n",
      "tested_positive      0.598     0.576     0.587        85\n",
      "\n",
      "       accuracy                          0.701       231\n",
      "      macro avg      0.678     0.675     0.676       231\n",
      "   weighted avg      0.699     0.701     0.700       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tweaking tree parameters\n",
    "tree_clf_best = DecisionTreeClassifier(max_depth=15, min_samples_split=5, min_samples_leaf=3)\n",
    "tree_clf_best.fit(x_train, y_train)\n",
    "y_pred_tree_best = tree_clf_best.predict(x_test)\n",
    "print(classification_report(y_test, y_pred_tree_best, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29369d3",
   "metadata": {},
   "source": [
    "### Final Result\n",
    "\n",
    "The model with the best accuracy after parameter modification was the **Logistic Regression model** with a CA of **0.779**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
